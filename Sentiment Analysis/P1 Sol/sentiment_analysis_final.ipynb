{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3heK5W9WmSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40e465d-ee63-4962-986b-0723c85ba19e"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 200)\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.externals import joblib\n",
        "from sklearn import utils\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora\n",
        "from pprint import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model, save_model\n",
        "from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "import string\n",
        "import re\n",
        "import logging\n",
        "import pickle\n",
        "from multiprocessing import cpu_count\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm2FXnRLZmYH"
      },
      "source": [
        "def load_data():\n",
        "  file_path = '/content/drive/My Drive/nlp_dataset/P1_training.xlsx'\n",
        "  df = pd.read_excel(file_path)\n",
        "  file_path_test = '/content/drive/My Drive/nlp_dataset/P1_testing.xlsx'\n",
        "  df_test = pd.read_excel(file_path_test)\n",
        "  return (df,df_test)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCivGkIixnKO"
      },
      "source": [
        "class TextProcessing(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def remove_punctuations(self, input_text):\n",
        "    punctuations = string.punctuation\n",
        "    translation_tab = str.maketrans(punctuations, len(punctuations)*\" \")\n",
        "    return input_text.translate(translation_tab)\n",
        "\n",
        "  def remove_digits(self, input_text):\n",
        "    return re.sub('\\d+', '', input_text)\n",
        "  \n",
        "  def converto_lower(self, input_text):\n",
        "    return input_text.lower()\n",
        "\n",
        "  def fit(self, X, y=None, **fit_params):\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X, **transform_params):\n",
        "    return X.apply(self.remove_punctuations).apply(self.remove_digits).apply(self.converto_lower) #.apply(self.remove_stopwords).apply(self.stemming)    #.apply(self.lemmatization)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5mCn3jYFO-6"
      },
      "source": [
        "def tfidf_word_embedding(df):\n",
        "  clean_text = TextProcessing()\n",
        "  df_clean = clean_text.fit_transform(df['sentence'])\n",
        "  X_train = df_clean\n",
        "  y_train = df['label']\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "  tfidf_vectorizer.fit(X_train)\n",
        "  X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
        "  \n",
        "  #grid search for logistic regression\n",
        "  param_grid_LR = {'C':[0.01, 0.05, 0.25, 0.5, 0.1]}\n",
        "  clf_LR = GridSearchCV(LogisticRegression(max_iter = 500, class_weight='balanced'), param_grid=param_grid_LR)  \n",
        "  clf_LR.fit(X_train_tfidf, y_train)\n",
        "\n",
        "  model = LogisticRegression(C=clf_LR.best_params_['C'], max_iter=500, class_weight='balanced')\n",
        "  model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "  return (tfidf_vectorizer, model) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoAIrvI3DtM7"
      },
      "source": [
        "def compute_avg_vector(w2v_dict, sentence):\n",
        "  list_of_word_vectors = [w2v_dict[w] for w in sentence if w in w2v_dict.vocab.keys()]\n",
        "  if len(list_of_word_vectors) == 0:\n",
        "    result = [0.0]*300\n",
        "  else:\n",
        "    result = np.sum(list_of_word_vectors, axis=0)/len(list_of_word_vectors)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg8fKdiKTHGI"
      },
      "source": [
        "def glove_w2v_embedding(df):\n",
        "  clean_text = TextProcessing()\n",
        "  df_clean = clean_text.fit_transform(df['sentence'])\n",
        "  X_train, y_train = df_clean, df['label']\n",
        "\n",
        "  sentences = X_train.values.tolist()\n",
        "  tokenized_list = [simple_preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "  X_train_w2v = pd.Series(tokenized_list).apply(lambda x: \n",
        "                                              compute_avg_vector(glove_w2v_model, x))\n",
        "  X_train_w2v = pd.DataFrame(X_train_w2v.values.tolist(), index=X_train.index)\n",
        "\n",
        "  #grid search\n",
        "  param_grid_LR = {'C':[0.01, 0.05, 0.25, 0.5, 0.1]}\n",
        "  clf_LR = GridSearchCV(LogisticRegression(max_iter = 500, class_weight='balanced'), param_grid=param_grid_LR)\n",
        "  clf_LR.fit(X_train_w2v, y_train)\n",
        "  \n",
        "  model = LogisticRegression(C=clf_LR.best_params_['C'], max_iter=500, class_weight='balanced')\n",
        "  model.fit(X_train_w2v, y_train)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1I789LkbUQh"
      },
      "source": [
        "def universal_sentence_encoder(df, classification='lr', plot_training=False):\n",
        "\n",
        "  X_train, y_train = df['sentence'], df['label']\n",
        "  X_train_vectors = []\n",
        "  for r in X_train:\n",
        "    emb = embed([r])\n",
        "    sentence_emb = tf.reshape(emb, [-1]).numpy()\n",
        "    X_train_vectors.append(sentence_emb)\n",
        "\n",
        "  X_train_vectors = np.array(X_train_vectors)\n",
        "\n",
        "  if classification=='lr':\n",
        "    #grid search\n",
        "    param_grid_LR = {'C':[0.01, 0.05, 0.25, 0.5, 0.1]}\n",
        "    clf_LR = GridSearchCV(LogisticRegression(max_iter = 500, class_weight='balanced'), param_grid=param_grid_LR)\n",
        "    clf_LR.fit(X_train_vectors, y_train)\n",
        "    \n",
        "    model = LogisticRegression(C=clf_LR.best_params_['C'], max_iter=500, class_weight='balanced')\n",
        "    model.fit(X_train_vectors, y_train)\n",
        "\n",
        "  elif classification=='nn':\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(units=64, input_shape=(X_train_vectors.shape[1],),\n",
        "                                activation='relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.5))\n",
        "    model.add(keras.layers.Dense(units=32, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.5))\n",
        "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(0.001),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(\n",
        "    X_train_vectors, y_train, epochs=7, batch_size=32,\n",
        "    verbose=0, validation_split=0.1, shuffle=True)\n",
        "\n",
        "    if plot_training:\n",
        "      plt.plot(history.history['loss'], label='training data')\n",
        "      plt.plot(history.history['val_loss'], label='validation data')\n",
        "      plt.legend(loc=\"upper right\")\n",
        "      plt.show()\n",
        "\n",
        "      plt.plot(history.history['accuracy'], label='training data')\n",
        "      plt.plot(history.history['val_accuracy'], label='validation data')\n",
        "      plt.legend(loc=\"upper left\")\n",
        "      plt.show()\n",
        "\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atisTyWLGS6d"
      },
      "source": [
        "def evaluate_model(df_test, model, embedding='tfidf', classification_method='lr',\n",
        "                   vectorizer=None):\n",
        "  word_emb = {'tfidf':'TF-IDF', 'glove':'GLOVE WORD2VEC', 'use':'UNIVERSAL SENTENCE ENCODER'}\n",
        "  if embedding != 'use':\n",
        "    clean_text = TextProcessing()\n",
        "    df_test_clean = clean_text.fit_transform(df_test['sentence'])\n",
        "    X_test, y_test = df_test_clean, df_test['label']\n",
        "  \n",
        "  if embedding == 'tfidf':\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "    pred_classes = model.predict(X_test_tfidf)\n",
        "    \n",
        "  elif embedding == 'glove':\n",
        "    test_sentences = X_test.values.tolist()\n",
        "    tokenized_testlist = [simple_preprocess(sentence) for sentence in test_sentences]\n",
        "    X_test_w2v = pd.Series(tokenized_testlist).apply(lambda x: \n",
        "                                                 compute_avg_vector(glove_w2v_model, x))\n",
        "    X_test_w2v = pd.DataFrame(X_test_w2v.values.tolist(), index=X_test.index)\n",
        "    pred_classes = model.predict(X_test_w2v)\n",
        "    \n",
        "  elif embedding == 'use':\n",
        "    X_test, y_test = df_test['sentence'], df_test['label']\n",
        "    X_test_vectors = []\n",
        "    for r in X_test:\n",
        "      emb = embed([r])\n",
        "      sentence_emb = tf.reshape(emb, [-1]).numpy()\n",
        "      X_test_vectors.append(sentence_emb)\n",
        "\n",
        "    X_test_vectors = np.array(X_test_vectors)\n",
        "    \n",
        "    if classification_method == 'lr':\n",
        "      pred_classes = model.predict(X_test_vectors)\n",
        "    elif classification_method == 'nn':\n",
        "      pred_classes = np.argmax(model.predict(X_test_vectors), axis=-1)\n",
        "      #pred_classes = model.predict_classes(X_test_vectors, verbose=0)\n",
        "\n",
        "  print(f'Model: {word_emb[embedding]}')\n",
        "  accuracy = accuracy_score(y_test, pred_classes)\n",
        "  precision = precision_score(y_test, pred_classes, average='weighted')\n",
        "  recall = recall_score(y_test, pred_classes, average='weighted')\n",
        "  f1 = f1_score(y_test, pred_classes, average='weighted')\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  print('Precision: %f' %precision)\n",
        "  print('Recall: %f' %recall)\n",
        "  print('F1 score: %f' %f1)\n",
        "\n",
        "  output_df = pd.DataFrame(columns=['sentence', 'gold_label', 'predicted_label'])\n",
        "  output_df = df_test\n",
        "  output_df['predicted_label'] = pred_classes\n",
        "  output_df.rename(columns = {'label':'gold_label'}, inplace = True) \n",
        "  print('\\nOUTPUT:')\n",
        "  print(output_df)\n",
        "  return output_df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s96U8Y7ILyOE"
      },
      "source": [
        "file_path1 = '/content/drive/My Drive/nlp_dataset/testing_output_tfidf.csv' \n",
        "file_path2 = '/content/drive/My Drive/nlp_dataset/testing_output_gloveW2V.csv' \n",
        "file_path3 = '/content/drive/My Drive/nlp_dataset/testing_output_USE_lr.csv' \n",
        "file_path4 = '/content/drive/My Drive/nlp_dataset/testing_output_USE_nn.csv' \n",
        "tfidf_model_file = '/content/drive/My Drive/nlp_dataset/tfidf_model.sav'\n",
        "glove_model_file = '/content/drive/My Drive/nlp_dataset/glove_model.sav'\n",
        "use_lr_model_file = '/content/drive/My Drive/nlp_dataset/use_lr_model.sav'\n",
        "use_nn_model_file = '/content/drive/My Drive/nlp_dataset/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3FkTE4nLJOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fc20be-604f-4422-a12b-d0b0e2a33659"
      },
      "source": [
        "df, df_test = load_data()\n",
        "tfidf_vectorizer, tfidf_model = tfidf_word_embedding(df)\n",
        "\n",
        "pickle.dump(tfidf_model, open(tfidf_model_file, 'wb'))\n",
        "\n",
        "#load model if saved \n",
        "#tfidf_model = pickle.load(open(tfidf_model_file, 'rb'))\n",
        "tfidf_output = evaluate_model(df_test, tfidf_model, embedding='tfidf', classification_method='lr', vectorizer=tfidf_vectorizer)\n",
        "tfidf_output.to_csv(file_path1, index=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: TF-IDF\n",
            "Accuracy: 0.5563689604685212\n",
            "Precision: 0.570978\n",
            "Recall: 0.556369\n",
            "F1 score: 0.563128\n",
            "\n",
            "OUTPUT:\n",
            "                                              sentence  gold_label  predicted_label\n",
            "0    even if the whole thing proves to be a creativ...           2                0\n",
            "1    , but isn't quite sure how to handle \" sam dee...           1                1\n",
            "2    ruby's close friend gretchen ( cuz ya can't ha...           2                2\n",
            "3    happy accidents is a romantic comedy filtered ...           2                2\n",
            "4    the film stars thandie newton , who was robbed...           2                1\n",
            "..                                                 ...         ...              ...\n",
            "678  somehow , with a considerable suspension of di...           1                2\n",
            "679  occasionally , the violence is slightly uncomf...           0                2\n",
            "680  what is perhaps most sensational about gods an...           2                2\n",
            "681  he earned that nomination with his touching pe...           2                2\n",
            "682  the iron giant , \" which was directed and co-w...           2                2\n",
            "\n",
            "[683 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LbFtLYfnwGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1667bc85-e805-400e-f76d-9e1b33425e59"
      },
      "source": [
        "#GLOVE \n",
        "glove_w2v_model = api.load('glove-wiki-gigaword-300')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=============================================-----] 90.9% 342.1/376.1MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAIxjHavVgI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf96db2e-ae87-4435-9847-1cb7052a7412"
      },
      "source": [
        "df, df_test = load_data()\n",
        "\n",
        "glove_model = glove_w2v_embedding(df)\n",
        "pickle.dump(glove_model, open(glove_model_file, 'wb'))\n",
        "\n",
        "#load model if saved \n",
        "#glove_model = pickle.load(open(glove_model_file, 'rb'))\n",
        "glove_output = evaluate_model(df_test, glove_model, 'glove', 'lr')\n",
        "glove_output.to_csv(file_path2, index=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: GLOVE WORD2VEC\n",
            "Accuracy: 0.5592972181551976\n",
            "Precision: 0.634280\n",
            "Recall: 0.559297\n",
            "F1 score: 0.585576\n",
            "\n",
            "OUTPUT:\n",
            "                                              sentence  gold_label  predicted_label\n",
            "0    even if the whole thing proves to be a creativ...           2                2\n",
            "1    , but isn't quite sure how to handle \" sam dee...           1                1\n",
            "2    ruby's close friend gretchen ( cuz ya can't ha...           2                2\n",
            "3    happy accidents is a romantic comedy filtered ...           2                2\n",
            "4    the film stars thandie newton , who was robbed...           2                2\n",
            "..                                                 ...         ...              ...\n",
            "678  somehow , with a considerable suspension of di...           1                2\n",
            "679  occasionally , the violence is slightly uncomf...           0                0\n",
            "680  what is perhaps most sensational about gods an...           2                2\n",
            "681  he earned that nomination with his touching pe...           2                2\n",
            "682  the iron giant , \" which was directed and co-w...           2                1\n",
            "\n",
            "[683 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsJPcqIrWRSw"
      },
      "source": [
        "#UNIVERSAL SENTENCE ENCODER\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F09EeNrpfFLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a427181-3ad5-4e1b-eade-ab6949681bc2"
      },
      "source": [
        "df, df_test = load_data()\n",
        "use_lr_model = universal_sentence_encoder(df, classification='lr')\n",
        "pickle.dump(use_lr_model, open(use_lr_model_file, 'wb'))\n",
        "\n",
        "#load model if saved \n",
        "#use_lr_model = pickle.load(open(use_lr_model_file, 'rb'))\n",
        "use_lr_output = evaluate_model(df_test, use_lr_model, 'use', 'lr')\n",
        "use_lr_output.to_csv(file_path3,index=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: UNIVERSAL SENTENCE ENCODER\n",
            "Accuracy: 0.6076134699853587\n",
            "Precision: 0.660863\n",
            "Recall: 0.607613\n",
            "F1 score: 0.625554\n",
            "\n",
            "OUTPUT:\n",
            "                                              sentence  gold_label  predicted_label\n",
            "0    even if the whole thing proves to be a creativ...           2                2\n",
            "1    , but isn't quite sure how to handle \" sam dee...           1                0\n",
            "2    ruby's close friend gretchen ( cuz ya can't ha...           2                1\n",
            "3    happy accidents is a romantic comedy filtered ...           2                2\n",
            "4    the film stars thandie newton , who was robbed...           2                2\n",
            "..                                                 ...         ...              ...\n",
            "678  somehow , with a considerable suspension of di...           1                0\n",
            "679  occasionally , the violence is slightly uncomf...           0                0\n",
            "680  what is perhaps most sensational about gods an...           2                2\n",
            "681  he earned that nomination with his touching pe...           2                2\n",
            "682  the iron giant , \" which was directed and co-w...           2                2\n",
            "\n",
            "[683 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMhcHj1PfsSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a0954b-b1b8-4d51-96d1-d0b01405a93e"
      },
      "source": [
        "#train model\n",
        "df, df_test = load_data()\n",
        "use_nn_model = universal_sentence_encoder(df, classification='nn', plot_training=False)\n",
        "save_model(use_nn_model, use_nn_model_file)\n",
        "#load model if saved \n",
        "#use_nn_model = load_model(use_nn_model_file)\n",
        "use_nn_output = evaluate_model(df_test, use_nn_model, 'use', 'nn') \n",
        "use_nn_output.to_csv(file_path4,index=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/nlp_dataset/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/nlp_dataset/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: UNIVERSAL SENTENCE ENCODER\n",
            "Accuracy: 0.6515373352855052\n",
            "Precision: 0.628719\n",
            "Recall: 0.651537\n",
            "F1 score: 0.630304\n",
            "\n",
            "OUTPUT:\n",
            "                                              sentence  gold_label  predicted_label\n",
            "0    even if the whole thing proves to be a creativ...           2                2\n",
            "1    , but isn't quite sure how to handle \" sam dee...           1                1\n",
            "2    ruby's close friend gretchen ( cuz ya can't ha...           2                1\n",
            "3    happy accidents is a romantic comedy filtered ...           2                2\n",
            "4    the film stars thandie newton , who was robbed...           2                2\n",
            "..                                                 ...         ...              ...\n",
            "678  somehow , with a considerable suspension of di...           1                0\n",
            "679  occasionally , the violence is slightly uncomf...           0                1\n",
            "680  what is perhaps most sensational about gods an...           2                2\n",
            "681  he earned that nomination with his touching pe...           2                2\n",
            "682  the iron giant , \" which was directed and co-w...           2                2\n",
            "\n",
            "[683 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elHHKlv4hP07"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}