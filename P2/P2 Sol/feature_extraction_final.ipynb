{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_extraction_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX1Ee-gf7jbT",
        "outputId": "a6f38368-6742-4807-b5d6-439a93ea2299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAGplHYq6-A0",
        "outputId": "99b0fecc-a019-432e-b7d3-d3e8349ad4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip install -U pywsd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pywsd\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/bb/427a49c461b0970c124509f77d2cd75bdca0daa746155c45d065f0af93e3/pywsd-1.2.4.tar.gz (26.8MB)\n",
            "\u001b[K     |████████████████████████████████| 26.8MB 149kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from pywsd) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from pywsd) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from pywsd) (1.1.2)\n",
            "Collecting wn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/f6/72db36e8afc977ae1a1cbb22afc77fd9b514e9bc6927ae8f4aae36665961/wn-0.0.23.tar.gz (31.6MB)\n",
            "\u001b[K     |████████████████████████████████| 31.6MB 111kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from pywsd) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pywsd) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pywsd) (2018.9)\n",
            "Building wheels for collected packages: pywsd, wn\n",
            "  Building wheel for pywsd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pywsd: filename=pywsd-1.2.4-cp36-none-any.whl size=26940455 sha256=6846b4b9b33ff385a024a12e1d5da2596a2bdfbe9b9ae372ce7478332b70a692\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/4d/d2/405b948047f7f3851f16ab9d893ce7c1a3010182900884536b\n",
            "  Building wheel for wn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wn: filename=wn-0.0.23-cp36-none-any.whl size=31792944 sha256=8ecd5b3ec033d8f63dcd7b7e2a6c2f180ec6ab62379f68cb86fbc19177a869a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/e3/c4/886021dbf4d758dc3cb9ddaa47d7d6fc895240d83f010e6305\n",
            "Successfully built pywsd wn\n",
            "Installing collected packages: wn, pywsd\n",
            "Successfully installed pywsd-1.2.4 wn-0.0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3heK5W9WmSL",
        "outputId": "e8edb456-1944-4eef-a75e-614e5e8132c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 200)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from pywsd import disambiguate, simple_lesk\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warming up PyWSD (takes ~10 secs)... "
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-02878a7a11ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpywsd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisambiguate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_lesk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pywsd/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpywsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallwords_wsd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisambiguate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msimple_lesk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is a foo bar sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'took {} secs.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pywsd/lesk.py\u001b[0m in \u001b[0;36msimple_lesk\u001b[0;34m(context_sentence, ambiguous_word, pos, lemma, stem, hyperhypo, stop, context_is_lemmatized, nbest, keepscore, normalizescore, from_cache)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                 from_cache=from_cache)\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# Disambiguate the sense in context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mcontext_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontext_is_lemmatized\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlemmatize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     return compare_overlaps(context_sentence, ss_sign, nbest=nbest,\n\u001b[1;32m    251\u001b[0m                             keepscore=keepscore, normalizescore=normalizescore)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pywsd/utils.py\u001b[0m in \u001b[0;36mlemmatize_sentence\u001b[0;34m(sentence, neverstem, keepWordPOS, tokenizer, postagger, lemmatizer, stemmer)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpostagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenn2morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         lemmas.append(lemmatize(word.lower(), pos, neverstem,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pywsd/tokenize.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    139\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFcuRNGlqfNU"
      },
      "source": [
        "#UNIVERSAL SENTENCE ENCODER\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmRMWJ9jf_9f"
      },
      "source": [
        "def load_data():\n",
        "  file_path = '/content/drive/My Drive/nlp_dataset/p2_train.csv'\n",
        "  df = pd.read_csv(file_path)\n",
        "  file_path_test = '/content/drive/My Drive/nlp_dataset/p2_test.csv'\n",
        "  df_test = pd.read_csv(file_path_test)\n",
        "  return (df,df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4UaamH1hA9S"
      },
      "source": [
        "def clean_data(df, df_test):\n",
        "  columns = ['precedent', 'question', 'subsequent', 'response', 'type']\n",
        "  df = df.iloc[:,5:]\n",
        "  df_test = df_test.iloc[:,5:]\n",
        "  df = df.fillna('')\n",
        "  df = df[columns].replace({'&gt;': '','<&quot;>': ''}, regex=True)\n",
        "  df_test = df_test.fillna('')\n",
        "  df_test = df_test[columns].replace({'&gt;': '','<&quot;>': ''}, regex=True)\n",
        "  return (df, df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxg6kH6shlBc"
      },
      "source": [
        "def sentiment_analyzer_scores(sentence):\n",
        "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "    score = sentiment_analyzer.polarity_scores(sentence)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L6AV2iMkklr"
      },
      "source": [
        "def create_train_test_set(df, df_test, features='baseline'):\n",
        "\n",
        "\n",
        "  columns = ['precedent', 'question', 'subsequent', 'response', 'type']\n",
        "  target = {'answered':0, 'attacked':1, 'irrelevant':2, 'agreed':3}\n",
        "  sentiments_scale = {'neu':1, 'neg':-2, 'pos':2, 'compound':0}\n",
        "  feature_columns = {0:'precedent', 1:'question', 2:'subsequent', 3:'response'}\n",
        "    \n",
        "\n",
        "\n",
        "  X_train, y_train = df.iloc[:,:-1], df['type']\n",
        "  y_train = y_train.map(target, na_action='ignore') \n",
        "  X_test, y_test = df_test.iloc[:,:-1], df_test['type']\n",
        "  y_test = y_test.map(target, na_action='ignore')\n",
        "\n",
        "  if features == 'baseline':\n",
        "    #creating training vectors\n",
        "    X_train_vectors = [[] for _ in range(len(X_train.columns))]  \n",
        "    for i in range(len(X_train.columns)): \n",
        "      for sentence in X_train[feature_columns[i]]:\n",
        "        wordsList = nltk.word_tokenize(sentence) \n",
        "        text_tagged = nltk.pos_tag(wordsList) \n",
        "        new_text = []\n",
        "        for word in text_tagged:\n",
        "          new_text.append(word[0] + \"/\" + word[1])\n",
        "        new_text = ' '.join(new_text)    \n",
        "        emb = embed([new_text])\n",
        "        sentence_emb = tf.reshape(emb, [-1]).numpy()\n",
        "        X_train_vectors[i].append(sentence_emb)\n",
        "\n",
        "    X_train_vectors = list(zip(X_train_vectors[0], X_train_vectors[1], X_train_vectors[2], X_train_vectors[3]))\n",
        "    for i in range(len(X_train_vectors)):\n",
        "      X_train_vectors[i] = [item for sublist in X_train_vectors[i] for item in sublist]\n",
        "    X_train_vectors = np.array(X_train_vectors)\n",
        "\n",
        "     \n",
        "    #creating testing vectors\n",
        "    X_test_vectors = [[] for _ in range(len(X_test.columns))]\n",
        "    for i in range(len(X_test.columns)): \n",
        "      for sentence in X_test[feature_columns[i]]:\n",
        "        wordsList = nltk.word_tokenize(sentence) \n",
        "        text_tagged = nltk.pos_tag(wordsList) \n",
        "        new_text = []\n",
        "        for word in text_tagged:\n",
        "          new_text.append(word[0] + \"/\" + word[1])\n",
        "        new_text = ' '.join(new_text)    \n",
        "        emb = embed([new_text])\n",
        "        sentence_emb = tf.reshape(emb, [-1]).numpy()\n",
        "        X_test_vectors[i].append(sentence_emb)\n",
        "\n",
        "    X_test_vectors = list(zip(X_test_vectors[0], X_test_vectors[1], X_test_vectors[2], X_test_vectors[3]))\n",
        "    for i in range(len(X_test_vectors)):\n",
        "      X_test_vectors[i] = [item for sublist in X_test_vectors[i] for item in sublist]\n",
        "    X_test_vectors = np.array(X_test_vectors)\n",
        "\n",
        "  if features == 'additional':\n",
        "    X_train_vectors = [[] for _ in range(len(X_train.columns))]\n",
        "    sentiment_score_response = []\n",
        "\n",
        "    for i in range(len(X_train.columns)): \n",
        "      for sentence in X_train[feature_columns[i]]:\n",
        "        sentence_ws = disambiguate(sentence)\n",
        "        wordsList = nltk.word_tokenize(sentence) \n",
        "        text_tagged = nltk.pos_tag(wordsList) \n",
        "        \n",
        "        new_text = []\n",
        "        new_text2 = []\n",
        "        idx = 0\n",
        "        for word in text_tagged:\n",
        "          new_text.append(word[0] + \"/\" + word[1] + \"/\" + (sentence_ws[idx][1].name() if (sentence_ws[idx] and sentence_ws[idx][1]) else ''))\n",
        "          idx += 1\n",
        "\n",
        "        new_text = ' '.join(new_text)\n",
        "        \n",
        "        emb = embed([new_text])\n",
        "        sentence_emb = tf.reshape(emb, [-1]).numpy()\n",
        "        X_train_vectors[i].append(sentence_emb)\n",
        "        \n",
        "        if i == 3:\n",
        "          dic = sentiment_analyzer_scores(sentence)\n",
        "          sentiment_score_response.append([dic['neg'], dic['neu'], dic['pos'], dic['compound']])\n",
        "\n",
        "\n",
        "    X_train_vectors = list(zip(X_train_vectors[0], X_train_vectors[1], X_train_vectors[2], X_train_vectors[3]))\n",
        "    \n",
        "    for i in range(len(X_train_vectors)):\n",
        "      X_train_vectors[i] = [item for sublist in X_train_vectors[i] for item in sublist]\n",
        "    \n",
        "    for i in range(len(X_train_vectors)):\n",
        "      X_train_vectors[i] += sentiment_score_response[i]\n",
        "    \n",
        "    X_train_vectors = np.array(X_train_vectors)\n",
        "\n",
        "\n",
        "    #testing data set\n",
        "    X_test_vectors = [[] for _ in range(len(X_test.columns))]\n",
        "    sentiment_score_response = []\n",
        "\n",
        "    for i in range(len(X_test.columns)): \n",
        "      for sentence in X_test[feature_columns[i]]:\n",
        "        sentence_ws = disambiguate(sentence)\n",
        "        wordsList = nltk.word_tokenize(sentence) \n",
        "        text_tagged = nltk.pos_tag(wordsList) \n",
        "        \n",
        "        new_text = []\n",
        "        new_text2 = []\n",
        "        idx = 0\n",
        "        for word in text_tagged:\n",
        "          new_text.append(word[0] + \"/\" + word[1] + \"/\" + (sentence_ws[idx][1].name() if (sentence_ws[idx] and sentence_ws[idx][1]) else ''))\n",
        "          idx += 1\n",
        "\n",
        "        new_text = ' '.join(new_text)\n",
        "        \n",
        "        emb = embed([new_text])\n",
        "        sentence_emb = tf.reshape(emb, [-1]).numpy()\n",
        "        X_test_vectors[i].append(sentence_emb)\n",
        "        \n",
        "        if i == 3:\n",
        "          dic = sentiment_analyzer_scores(sentence)\n",
        "          sentiment_score_response.append([dic['neg'], dic['neu'], dic['pos'], dic['compound']])\n",
        "\n",
        "    X_test_vectors = list(zip(X_test_vectors[0], X_test_vectors[1], X_test_vectors[2], X_test_vectors[3]))\n",
        "    for i in range(len(X_test_vectors)):\n",
        "      X_test_vectors[i] = [item for sublist in X_test_vectors[i] for item in sublist]\n",
        "    \n",
        "    for i in range(len(X_test_vectors)):\n",
        "      X_test_vectors[i] += sentiment_score_response[i]\n",
        "    X_test_vectors = np.array(X_test_vectors)\n",
        "\n",
        "  return ((X_train_vectors, y_train), (X_test_vectors, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz_CuJYknAuy"
      },
      "source": [
        "def train_model(X_train, y_train, classifier='SVC', plot_training=False):\n",
        "  if classifier == 'SVC':\n",
        "    #SVC Grid Search\n",
        "    '''\n",
        "    param_grid_SVC = {'C': [0.1, 1, 10, 100],  \n",
        "                  'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
        "                  'kernel': ['rbf', 'linear']}  \n",
        "\n",
        "    clf_SVC = GridSearchCV(SVC(), param_grid_SVC)\n",
        "    clf_SVC.fit(X_train_vectors, y_train)\n",
        "    print(\"Best parameter: {}, Best score: {}\".format(clf_SVC.best_params_, clf_SVC.best_score_))\n",
        "    '''\n",
        "\n",
        "    model = SVC(C=10, kernel='rbf', gamma=1)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "  elif classifier == 'NN':\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(units=128, input_shape=(X_train.shape[1],),\n",
        "                                activation='relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.5))\n",
        "    model.add(keras.layers.Dense(units=64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.5))\n",
        "    model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(0.01),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(\n",
        "    X_train, y_train, epochs=9, batch_size=32,\n",
        "    verbose=0, validation_split=0.1, shuffle=True)\n",
        "\n",
        "    if plot_training:\n",
        "      plt.plot(history.history['loss'], label='training data')\n",
        "      plt.plot(history.history['val_loss'], label='validation data')\n",
        "      plt.legend(loc=\"upper right\")\n",
        "      plt.show()\n",
        "\n",
        "      plt.plot(history.history['accuracy'], label='training data')\n",
        "      plt.plot(history.history['val_accuracy'], label='validation data')\n",
        "      plt.legend(loc=\"upper left\")\n",
        "      plt.show()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5uR47UepjrQ"
      },
      "source": [
        "def test_model(model, X_test, y_test, classifier='SVC'):\n",
        "  if classifier == 'SVC':\n",
        "    pred_classes = model.predict(X_test)\n",
        "  elif classifier == 'NN':\n",
        "    pred_classes = np.argmax(model.predict(X_test), axis=-1)\n",
        "  accuracy = accuracy_score(y_test, pred_classes)\n",
        "  precision = precision_score(y_test, pred_classes, average='weighted')\n",
        "  recall = recall_score(y_test, pred_classes, average='weighted')\n",
        "  f1 = f1_score(y_test, pred_classes, average='weighted')\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  print('Precision: %f' %precision)\n",
        "  print('Recall: %f' %recall)\n",
        "  print('F1 score: %f' %f1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVPBxH14hpvd"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT0wUYL4hbQh"
      },
      "source": [
        "df, df_test = load_data()\n",
        "df, df_test = clean_data(df, df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylYG3smSjK7Z"
      },
      "source": [
        "Baseline features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmRNMO-qqPvf"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = create_train_test_set(df, df_test, features='baseline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TgOGLU6qYqs"
      },
      "source": [
        "model_svc = train_model(X_train, y_train, classifier='SVC')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8qqhPODtZ28"
      },
      "source": [
        "test_model(model_svc, X_test, y_test, classifier='SVC')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyx_QLR3ti8U"
      },
      "source": [
        "#model_nn = train_model(X_train, y_train, classifier='NN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI0cnGP-uxSY"
      },
      "source": [
        "#test_model(model_nn, X_test, y_test, classifier='NN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmU5vedojEt4"
      },
      "source": [
        "Baseline + Additional features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Tpmf6fv0C7"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = create_train_test_set(df, df_test, features='additional')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTFX-UJ037V_"
      },
      "source": [
        "model_svc = train_model(X_train, y_train, classifier='SVC')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYDaZTjH7JSU"
      },
      "source": [
        "test_model(model_svc, X_test, y_test, classifier='SVC')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}